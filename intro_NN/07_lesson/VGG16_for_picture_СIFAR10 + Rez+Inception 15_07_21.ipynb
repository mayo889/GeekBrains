{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ah7dy1kFL2H9"
   },
   "source": [
    "# Распознавание объектов на изображениях "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0pyYuxbg0wK"
   },
   "source": [
    "Грузим библиотеки (куча лишних, долго было вычищать)\n",
    "точно нужны слои,  optimizer , cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivmw9St3nFnx"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import  BatchNormalization\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Conv2D\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from google.colab import files\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline \n",
    "\n",
    "from tensorflow.keras import utils\n",
    "# Количество классов изображений\n",
    "nb_classes = 10\n",
    "# Названия классов из набора данных CIFAR-10\n",
    "classes=['самолет', 'автомобиль', 'птица', 'кот', 'олень', 'собака', 'лягушка', 'лошадь', 'корабль', 'грузовик']\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "y_train10 = utils.to_categorical(y_train, nb_classes)\n",
    "\n",
    "y_test10 = utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "#x_train=np.broadcast_to(x_train[..., None],(60000,28,28,3))\n",
    "#x_test=np.broadcast_to(x_test[..., None],(10000,28,28,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Odp_ZArlhTz9"
   },
   "source": [
    "задаем размер входа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OL9nxCJNy9lm"
   },
   "outputs": [],
   "source": [
    "# Размер изображений  CIFAR10\n",
    "img_width, img_height, chanel_n = 32, 32, 3\n",
    "#img_width, img_height, chanel_n = 28, 28, 3\n",
    "\n",
    "print(x_train.shape)\n",
    "#пакет для генератора\n",
    "batch_size = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xt3mwnU7GVP8"
   },
   "source": [
    "#Загрузка VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSqDL3bZzGAx"
   },
   "outputs": [],
   "source": [
    "# Загружаем сеть VGG16 без части, которая отвечает за классификацию\n",
    "base_model = applications.VGG16(weights ='imagenet', include_top=False, input_shape=(img_width, img_height, chanel_n ))\n",
    "\n",
    "base_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aj9SH91ehdUm"
   },
   "source": [
    "Делаем генератор преобразования объекта - только масштаб значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EN7nYHUH0DLa"
   },
   "outputs": [],
   "source": [
    "# Создаем генератор данных для обучения\n",
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "\n",
    "train_generator = datagen.flow(\n",
    "    x_train,y_train10,\n",
    "    batch_size=batch_size,\n",
    "    )\n",
    "# Создаем генератор данных для валидации\n",
    "val_generator =  datagen.flow(\n",
    "    x_test,y_test10,\n",
    "    batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "test_generator = datagen.flow(\n",
    "    x_test,y_test10,\n",
    "    batch_size=batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwX8VwHKGgfx"
   },
   "source": [
    "# Свой классификатор "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDcjFVOaeMGf"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "layer_size = 10\n",
    "models = Sequential()\n",
    "# Добавляем в модель сеть VGG16 вместо слоя\n",
    "models.add(Conv2D(128,(3,3), activation='sigmoid',input_shape = (32,32,3))) # только свертка\n",
    "models.add(BatchNormalization())\n",
    "for i in range(layer_size):\n",
    "    models.add(Conv2D(128,(3,3), activation='sigmoid')) # только свертка\n",
    "    models.add(BatchNormalization())\n",
    "\n",
    "models.add(Flatten()) # векторим вход\n",
    "models.add(Dense(100,activation='relu'))\n",
    "models.add(Dense(10,activation='softmax'))\n",
    "\n",
    "models.summary()\n",
    "\n",
    "print(models.layers[-1].input_shape)\n",
    "\n",
    "print(models.layers[-1].output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxV4zueVjQrr"
   },
   "source": [
    "Посмотрим, что вышло"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oq7y-ngeNYQe"
   },
   "outputs": [],
   "source": [
    "# компилияция модели\n",
    "models.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "to6ouJLaNrqc"
   },
   "outputs": [],
   "source": [
    "hist = models.fit(x=x_train,y=y_train10, epochs=2, batch_size=128, validation_data=(x_test, y_test10), verbose=1)\n",
    "\n",
    "test_score = models.evaluate(x_test, y_test10)\n",
    "print(\"Test loss {:.4f}, accuracy {:.2f}%\".format(test_score[0], test_score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSY2PvY8QxzV"
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['val_accuracy'],'r',label='val_accuracy')\n",
    "plt.plot(hist.history['accuracy'],'g',label='train_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5goLguw0RoYA"
   },
   "outputs": [],
   "source": [
    "predict1=models.predict(x_train[:20])\n",
    "print(predict1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6FRJqYsVwq0"
   },
   "source": [
    "Посмотрим , что распознается"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjfZMLmtSkgh"
   },
   "outputs": [],
   "source": [
    "n=1\n",
    "\n",
    "plt.imshow(x_train[n][:,:,:])\n",
    "plt.title(classes[np.argmax(predict1[n,:])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B24PXjO7TEww"
   },
   "outputs": [],
   "source": [
    "predict1_test=models.predict(x_test[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulRE-2L7T9U6"
   },
   "source": [
    "Сделаем немного тестов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdYSNCyGTjXk"
   },
   "outputs": [],
   "source": [
    "n=5\n",
    "\n",
    "plt.imshow(x_test[n][:,:,:])\n",
    "plt.title(classes[np.argmax(predict1_test[n,:])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJ1F3LkkTqBr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlvKXPLWxT8s"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Add , Input,  Dense, Dropout, Conv2D, MaxPool2D, Flatten, BatchNormalization\n",
    "import tensorflow\n",
    "from tensorflow.keras.utils import plot_model,to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ym8k1LI-xMLD"
   },
   "outputs": [],
   "source": [
    "# Создадим экземпляр оптимизатора. \n",
    "optimizer = tensorflow.keras.optimizers.SGD(learning_rate=1e-3) \n",
    "# Instantiate a loss function. \n",
    "loss_fn = tensorflow.keras.losses.MeanSquaredError() #берем, т.к. виднее изменения(from_logits=True) \n",
    "# Подготовим тренировочный датасет. \n",
    "batch_size = 64 \n",
    "epochs=2  # учим немного, т.к. задача посмотреть , что происходит с градиентом\n",
    "train_dataset = tensorflow.data.Dataset.from_tensor_slices((x_train[:1000,:,:,:], to_categorical(y_train[:1000,0]))) \n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size) \n",
    "\n",
    "def my_train(model=None, epochs=None,batch_size=64,train_dataset=train_dataset ):\n",
    "    # Итерируем по эпохам.\n",
    "    grad_log=[]\n",
    "    w_log=[]\n",
    "\n",
    "    for epoch in range(epochs): \n",
    "        print('Начинаем эпоху %d' % (epoch,)) \n",
    "        # Итерируем по пакетам в датасете. \n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset): \n",
    "            # Откроем GradientTape чтобы записать операции # выполняемые во время прямого прохода, \n",
    "            #включающего автодифференцирование. \n",
    "            with tensorflow.GradientTape() as tape: \n",
    "        # Запустим прямой проход слоя. \n",
    "        # Операции применяемые слоем к своим \n",
    "        # входным данным будут записаны \n",
    "        # на GradientTape. \n",
    "        logits = model(x_batch_train) \n",
    "        # Logits для пакета  - выходы модели \n",
    "        # Вычислим значение потерь для этого пакета. \n",
    "        y_s=y_batch_train.numpy().shape\n",
    "        y  = tensorflow.reshape( y_batch_train,shape=(y_s[0],10,1))\n",
    "        l = tensorflow.reshape( logits,shape=(y_s[0],10,1))\n",
    "        # вызываем лосс\n",
    "        loss_value = loss_fn(y, l) \n",
    "        # Используем gradient tape для автоматического извлечения градиентов \n",
    "        # обучаемых переменных относительно потерь. \n",
    "        grads = tape.gradient(loss_value, model.trainable_weights) \n",
    "        g_g=[]\n",
    "        w_w =[]\n",
    "        # пишем логи для сохранения значений градиента по одному ядру из слоя (осредняем по абсолютным значениям) и \n",
    "        # веса по одной цепи (0-й канал распространения активности)\n",
    "        for g_s in grads:\n",
    "            if len(g_s.numpy().shape)==1:\n",
    "                g_g.append(g_s.numpy()[0])\n",
    "            if len(g_s.numpy().shape)==2:\n",
    "                g_g.append(np.mean(np.abs((g_s.numpy()[:,0])))) \n",
    "            if len(g_s.numpy().shape)==3:\n",
    "                g_g.append(np.mean(np.abs(g_s.numpy()[:,:,0]))) \n",
    "            if len(g_s.numpy().shape)==4:\n",
    "                g_g.append(np.mean(np.abs(g_s.numpy()[:,:,0,0])) ) \n",
    "\n",
    "        for w_s in model.trainable_weights:\n",
    "            if len(w_s.numpy().shape)==1:\n",
    "                w_w.append(w_s.numpy()[0])\n",
    "            if len(w_s.numpy().shape)==2:\n",
    "                w_w.append(w_s.numpy()[0,0])\n",
    "            if len(w_s.numpy().shape)==3:\n",
    "                w_w.append(w_s.numpy()[0,0,0])\n",
    "            if len(w_s.numpy().shape)==4:\n",
    "                w_w.append(w_s.numpy()[0,0,0,0])\n",
    "\n",
    "        # добавляем текущие логи по слоям к общей записи\n",
    "        grad_log.append(g_g)\n",
    "        w_log.append(w_w)\n",
    "        # Выполним один шаг градиентного спуска обновив # значение переменных минимизирующих потери. \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) \n",
    "        # Пишем лог каждые 1000 пакетов. \n",
    "        if step % 1000 == 0: \n",
    "            print('Потери на обучении (для одного пакета) на шаге %s: %s' % (step, float(loss_value))) \n",
    "            print('Уже увидели: %s примеров' % ((step + 1) * batch_size))\n",
    "    grad_log=np.array(grad_log)\n",
    "    w_log=np.array(w_log)\n",
    "\n",
    "    return grad_log, w_log, model, x_batch_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVJALHfUx2Pz"
   },
   "outputs": [],
   "source": [
    "def visual_grad(grad_log=None, numb_layer=-1):\n",
    "    s_g = grad_log.shape\n",
    "    grad_log=grad_log/np.max(grad_log,axis=1).reshape((s_g[0],1))\n",
    "    #print(grad_log[0,:])\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.title('Градиент по слоям')\n",
    "    plt.xlabel('№ layer')\n",
    "    plt.ylabel('grad')\n",
    "    plt.grid()\n",
    "    plt.plot(np.abs(grad_log[0,:numb_layer]),label='step 0')\n",
    "    plt.plot(np.abs(grad_log[s_g[0] // 10,:numb_layer]),label='step '+str(s_g[0] // 10))\n",
    "    plt.plot(np.abs(grad_log[s_g[0] // 3,:numb_layer]),label='step '+str(s_g[0] // 3))\n",
    "    plt.plot(np.abs(grad_log[s_g[0]-1 ,:numb_layer]),label='step  '+str(s_g[0] ))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Vi1HaIk2kzV"
   },
   "outputs": [],
   "source": [
    "grad_log,w_log,models, x_batch_train = my_train( model=models, epochs=1,batch_size=10 , train_dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueqsBxKm3BZu"
   },
   "outputs": [],
   "source": [
    "visual_grad(grad_log=grad_log, numb_layer=grad_log.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrBZFZEIw5bh"
   },
   "source": [
    "ПОстроим сложный блок (Rez-Inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDwQHDqSx-8U"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(32,32,3), name='CIFAR10rez') \n",
    "\n",
    "x = Conv2D(64,3, activation='relu', name='conv_1')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64,3, activation='relu', name='conv_2')(x) \n",
    "# можно добавлять блоки сверток, но следует следить за размером\n",
    "# padding ='valid' - размер выходов постепенно уменьшается \n",
    "#ResNet - блок\n",
    "x_shortcut = x # +x\n",
    "x = Conv2D(64,3, activation='relu', padding='same',name='conv_31')(x) # H\n",
    "x = Conv2D(64,3, activation='relu', padding='same',name='conv_41')(x) \n",
    "x = Add()([x, x_shortcut]) # F = H +x\n",
    "x = BatchNormalization()(x)\n",
    "# нелинейная трансформация и уменьшение размера в 2 раза\n",
    "x = MaxPool2D()(x) \n",
    "x_shortcut = x\n",
    "x = Conv2D(64,3, activation='relu', padding='same', name='conv_32')(x) \n",
    "x = Conv2D(64,3, activation='relu', padding='same', name='conv_42')(x)\n",
    "x = Add()([x, x_shortcut])\n",
    "x = BatchNormalization()(x)\n",
    "x_shortcut = x\n",
    "\n",
    "x = Conv2D(64,3, activation='relu', padding='same', name='conv_3')(x) \n",
    "x = Conv2D(64,3, activation='relu',  padding='same', name='conv_4')(x)\n",
    "x = Add()([x, x_shortcut])  \n",
    "x = MaxPool2D((2,2),(2,2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu', name='dense_1')(x)\n",
    "\n",
    "\n",
    "outputs = Dense(10,activation='sigmoid' ,name='predictions')(x) \n",
    "model1 = Model(inputs=inputs, outputs=outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFo1eaI60R13"
   },
   "outputs": [],
   "source": [
    "plot_model(model1,to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iO7mXpbyMdP"
   },
   "outputs": [],
   "source": [
    "grad_log,w_log,model1, x_batch_train= my_train( model=model1, epochs=1,batch_size=1000 , train_dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BouWnobtxqlk"
   },
   "outputs": [],
   "source": [
    "visual_grad(grad_log=grad_log, numb_layer=grad_log.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uj31dpFdykE0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import concatenate \n",
    "\n",
    "\n",
    "inputs = Input(shape=(32,32,3), name='cifar_inception') \n",
    "\n",
    "x = Conv2D(64,3, activation='relu', name='conv_1')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64,3, activation='relu', name='conv_2')(x) \n",
    "# можно добавлять блоки сверток, но следует следить за размером\n",
    "# padding ='valid' - размер выходов постепенно уменьшается \n",
    "#Inception - блок\n",
    "x_shortcut = x\n",
    "x0 = Conv2D(64,1, activation='relu', padding='same',name='conv_131')(x) \n",
    "x1 = Conv2D(64,1, activation='relu', padding='same',name='conv_231')(x) \n",
    "x1 = Conv2D(64,5, activation='relu', padding='same',name='conv_341')(x1) \n",
    "x2 = Conv2D(64,1, activation='relu', padding='same',name='conv_431')(x) \n",
    "x2 = Conv2D(64,3, activation='relu', padding='same',name='conv_541')(x2) \n",
    "x3 = Conv2D(64,3, activation='relu', padding='same',name='conv_631')(x) \n",
    "x3 = Conv2D(64,1, activation='relu', padding='same',name='conv_741')(x3) \n",
    "x = concatenate([x0,x1,x2,x3],axis=-1)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64,3, activation='relu', padding='same', name='conv_ee32')(x) \n",
    "x = Add()([x, x_shortcut])\n",
    "# нелинейная трансформация и уменьшение размера в 2 раза\n",
    "x = MaxPool2D()(x) \n",
    "x_shortcut = x\n",
    "x = Conv2D(64,3, activation='relu', padding='same', name='conv_32')(x) \n",
    "x = Conv2D(64,3, activation='relu', padding='same', name='conv_42')(x)\n",
    "x = Add()([x, x_shortcut])\n",
    "x = BatchNormalization()(x)\n",
    "x_shortcut = x\n",
    "x = Conv2D(64,3, activation='relu', padding='same',name='conv_33')(x) \n",
    "x = Conv2D(64,3, activation='relu', padding='same',name='conv_43')(x)  \n",
    "x = Add()([x, x_shortcut])  \n",
    "x = BatchNormalization()(x)\n",
    "x_shortcut = x\n",
    "x = Conv2D(64,3, activation='relu', padding='same', name='conv_3')(x) \n",
    "x = Conv2D(64,3, activation='relu',  padding='same', name='conv_4')(x)\n",
    "x = Add()([x, x_shortcut])  \n",
    "x = MaxPool2D((2,2),(2,2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu', name='dense_1')(x) \n",
    " \n",
    "\n",
    "outputs = Dense(10,activation='sigmoid' ,name='predictions')(x) \n",
    "model1 = Model(inputs=inputs, outputs=outputs) \n",
    "\n",
    "grad_log,w_log,model1, x_batch_train= my_train( model=model1, epochs=1,batch_size=batch_size , train_dataset=train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLH3nnRB2Fgw"
   },
   "outputs": [],
   "source": [
    "plot_model(model1,to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHvAQqCd13ym"
   },
   "outputs": [],
   "source": [
    "visual_grad(grad_log=grad_log, numb_layer=grad_log.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiydaLdV2Tfb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VGG16_for_picture_СIFAR10 + Rez+Inception .ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1QUp8kvQYqFqtJoocoRSMxsEQFLgp_WJI",
     "timestamp": 1611852287987
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
