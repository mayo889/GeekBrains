{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtrpTCq95eOC"
   },
   "source": [
    "# Введение в искусственные нейронные сети\n",
    "# Урок 7. Дектирование объектов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2dU0-265eOE"
   },
   "source": [
    "## Содержание методического пособия:\n",
    "\n",
    "\n",
    "<ol>\n",
    "<li>Что такое дектирование объектов на изображении</li>\n",
    "<li>Виды архитектур для дектирования объектов на изображении</li>\n",
    "<li>Практический пример сегментации</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xd2MRCG85eOF"
   },
   "source": [
    "## Что такое детектирование объектов на изображении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d_wRv5M5eOG"
   },
   "source": [
    "Дектирование объектов на изображениях, пожалуй одна из самых сложных и самых полезных задач, которую может решать компьютерное зрение. \n",
    "\n",
    "Сложность проистекает из того факта, что нам нужно не только опредилить какому классу пренадлежат объекты на изображении, отделить одни экзепляры классов от других, но и найти их месторасположение на изображении.\n",
    "\n",
    "Полезность решения данного задачи связана с тем фактом, что она приближена к возможностям человеческого зрения и восприятия. Мы смотрим на окружающий мир, видем те или иные объекты и понимаем где они находятся. А понимая где они находятся мы можем понимать как они движутся. Нейронные сети решающее подобные задачи могут применяться в самом широком спектре областей - начиная от систем видеонаблюдения до полностью автоматизированных магазинов наподобие Amazon Go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUdlWEK5eOJ"
   },
   "source": [
    "В силу того, что решение данной задачи существенно отличается от задачи классификации изображений, существенно отличаются и архитектуры нейронных сетей для этой задачи, которые мы разберем на данном уроке. В первую очередь мы укажем на отличия в датасетах и метриках. \n",
    "\n",
    "Для обучения подобных нейронных сетей нам нужно не только понимать какие классы присутсвуют на изображении, но и понимать где они находяться, поэтому в датасетах для object detection, выделяются также bounding box'ы для того, чтобы нейросеть могла корректировать свои предсказания месторасположения объектов по ходу обучения.\n",
    "\n",
    "Известными датасетами для object detection являются следующие:\n",
    "1. Pascal VOC(имеются разные версии данного датасета за разные годы)\n",
    "2. MS COCO(также есть разные версии данного датасета за разные годы)\n",
    "3. ImageNet(в ImageNet есть разметка не только для классификации, но и для других задач)\n",
    "\n",
    "\n",
    "Также стоит отметить, что если в случае с задачей классификации, нам было возможно оценить работу нейронной сети, просто определив верный или не верный класс был предсказан, то в случае с object detection нам нужно определить помимо классов насколько точно определено его местоположение на изображении.\n",
    "\n",
    "Популярной метрикой для решения этой задачи является - mAP (mean Average Precision). Она учитывает насколько совпал bounding box предсказонной нейронной сетью с действительностью,\n",
    "\n",
    "![4.png](images/01_01.png)\n",
    "Источник изображения: https://miro.medium.com/max/800/1*FrmKLxCtkokDC3Yr1wc70w.png\n",
    "\n",
    "\n",
    "а также засчет дополнительной комбинации формул понимает как много объектов правильно найдены на изображении и для сколь многих из них предсказан верный класс.\n",
    "\n",
    "\n",
    "![3.jpg](images/01_02.gif)\n",
    "Источник изображения: https://i.stack.imgur.com/NSaRb.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUhHUTsA5eOK"
   },
   "source": [
    "Теперь давайте перейдем к архитектурам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nqs1JLvx5eOL"
   },
   "source": [
    "## Виды архитектур для дектирования объектов на изображении.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7WsQEsl5eOM"
   },
   "source": [
    "Начиная с 2012 г., когда широкие круги специалистов в области компьютерного зрения стали использовать нейронные сети, для тех задач которые прежде решали классические алгоритмы комп. зрения, появилось и продолжает появляться множество архитектур предназначенных для object detection. Мы осветим несколько архитектур, которые стали важными вехами с одной стороны, а с другой стороны их модификации являются на данный момент лучшими способами для решения задачи object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyzK2gEq5eON"
   },
   "source": [
    "![timeline_obj_detect.png](images/01_03.png)\n",
    "Источник изображения: Recent Advances in Deep Learning for Object Detection. Xiongwei Wu, Doyen Sahoo, Steven C.H. Hoi 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrSDvAKS5eOO"
   },
   "source": [
    "# R-CNN #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcVVnBz15eOP"
   },
   "source": [
    "R-CNN - это одна из первых архитектур для решения задачи object detection на основе сверточных нейронных сетей.\n",
    "Данная архитектура состоит из трех частей:\n",
    "\n",
    "1. Классический алгоритм комп. зрения, который находит области изображения на которых потенциально могут содержаться объекты\n",
    "\n",
    "2. Сверточная нейронная сеть, которая запускается по отдельности в каждом найденном регионе и выдает набор feateres.\n",
    "\n",
    "3. SVM алгоритм который обучается на этих features определять те или иные классы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIXEj8U35eOQ"
   },
   "source": [
    "![r-cnn.png](images/01_r-cnn.png)\n",
    "Источник изображения: https://arxiv.org/pdf/1311.2524.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAtlXx075eOQ"
   },
   "source": [
    "Данная нейросеть работала медленно. Это происходило из-за того, что приходилось многократно запускать сверточную часть на множесте областей изображения. На одно изображение уходили десятки секунд. Также данная архитектура не могла обучаться целиком, а обучалась по отдельности. Кроме того обучалась данная нейросеть тоже медленно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-IS6-8_5eOa"
   },
   "source": [
    "# Fast R-CNN\n",
    "\n",
    "Данная архитектура появилась в 2015 г. и была призвана решить упомянутые выше проблемы R-CNN. Fast R-CNN состоял уже из следующих компонентов:\n",
    "\n",
    "1. Сверточная нейронная сеть которая запускается один раз на всем изображении.\n",
    "\n",
    "2. ROI - компонент который позволяет искать области где могут находиться объекты не на первоначальных пикслелях изображения, а на карте признаков, которую выдает сверточная часть данной архитектуры.\n",
    "\n",
    "3. Полносвязный слой, который делает непосредственно предсказание\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2v6WAVI5eOc"
   },
   "source": [
    "![fast_r-cnn.png](images/01_fast-rcnn.png)\n",
    "Источник изображения: https://arxiv.org/pdf/1504.08083.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hfs0Ib_o5eOe"
   },
   "source": [
    "Данная архитектура ускорила и обучение и скорость работы изначально архитектуры R-CNN существенно. Теперь одно изображение обрабатывалось нейронной сетью за 2 секунды. Однако это по-прежнему было далеко от real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbxMbuH15eOf"
   },
   "source": [
    "# Faster R-CNN #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1G6E_z85eOh"
   },
   "source": [
    "Данная архитектура появилась в 2016 году и является как одной из самых точных на сегодняшний день, так и относительно быстрой, время обработки ею изображения занимает меньше секунды."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Enic02Ss5eOi"
   },
   "source": [
    "Она состоит из следующих компонентов:\n",
    "1. Сверточная нейронная сеть\n",
    "2. Нейронная сеть корректирующая работу ROI, выбирающего регионы в карте features получающейся в ходе работы сверточной части.\n",
    "3. Полносвязные слои осуществляющие предсказание."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aq5A0Ctj5eOl"
   },
   "source": [
    "![faster_r-cnn.png](images/01_faster-rcnn.png)\n",
    "Источник изображения: https://arxiv.org/pdf/1506.01497.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXjm4NnX5eOn"
   },
   "source": [
    "Данная архитектура позволяла окончательно позволили обучать нейроннуют сеть для object detection end-to-end, т.е. нейронная сеть училась и определять классы объектов и корректировать предсказания местоположения объектов(bounding box'ы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFQrI8Nz5eOo"
   },
   "source": [
    "# YOLO (You Only Look Once) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGZ6cIVu5eOq"
   },
   "source": [
    "Несмотря на то, что Faster R-CNN работала относительно быстро и с высокой точностью, она все таки была тяжеловестна и недостаточно быстра для многих задач. Все вышеприведенные архитектуры относиться к двухстадийным архитектурам, т.е. мы отдельно находим объекты и отдельно их классифицируем.\n",
    "\n",
    "Архитектура YOLO, появившаяся в 2016 г. является первой популярной одностадийной архитектурой. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmmQz6MC5eOs"
   },
   "source": [
    "![YOLO.png](images/01_yolo.png)\n",
    "Источник изображения: https://arxiv.org/pdf/1506.02640.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkTLNyS45eOu"
   },
   "source": [
    "Данная нейронная сеть разбивает все изображение на фиксированное количество квадратов. Затем за один проход она пытается предсказать в разных комбинациях этих квадратов те или иные классы. Таким образом данная нейронная сеть несколько теряет в точности, но существенно приобретает в скорости работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMCgNQX15eOv"
   },
   "source": [
    "# SSD (Single Shot MultiBox Detector) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSb2p5VA5eOy"
   },
   "source": [
    "Данная архитектура появилась в 2016 г. и различные ее модификации являются одними из самыми применяемых на практике.\n",
    "\n",
    "Она также как и YOLO является одностадийным дектором, также как и YOLO пытается на лету определить boundig box'ы и классы, но считывание ее результатов происходит на разных масштабах в конце нейронной сети, подобному так как это происходит в архитектуре FPN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAoLxI1Z5eOz"
   },
   "source": [
    "![SSD.png](images/01_ssd.png)\n",
    "Источник изображения: https://arxiv.org/pdf/1512.02325.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ufo-00oW5eO2"
   },
   "source": [
    "### Mask R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mgtGF1C5eO4"
   },
   "source": [
    "Mask R-CNN позволяет решить задачу instance segmentation, которую мы рассматривали на предыдущем уроке. Данная нейросеть сначала посредством Fast R-CNN находит нужны объекты на изображении, а затем посредством сегментации накладывает на них маску."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZRIbJdH5eO5"
   },
   "source": [
    "![Mask_R-CNN.png](images/01_mask_rcnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xY6RAy-H5eO7"
   },
   "source": [
    "Источник изображения: https://miro.medium.com/max/1908/1*ui1roGvi_F77TY07PdaI8w.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZCM65CBt1CJ"
   },
   "source": [
    "В целом, архитектуру для решения задачи object detection нужно подбирать исходя из нужного вам сочетания точности распознования и скорости работы нейронной сети. \n",
    "\n",
    "Данная диаграмма показывая различные архитектуры с различными вариантами сверточной части для них, дает общее представление в этом вопросе - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hlrZAw65eO9"
   },
   "source": [
    "![compare_table.png](images/01_table.png)\n",
    "Источник изображения: https://cdn-images-1.medium.com/fit/t/3750/2265/1*xgBs8CZdf1AvaFz92ERB6A.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eGMFag-5eO_"
   },
   "source": [
    "## Практический пример "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1pCjWWR5ePB"
   },
   "source": [
    "Обучение нейронных сетей для object detection требует работы над большими датасетами и громоздкой архитектуры. Мы приведем пример того, как может быть определена нейросеть SSD на tensorflow 1.15. Полный код для обучения данной нейронной сети можно найти в данном репозитории - https://github.com/sergeyveneckiy/ssd-tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FD60EbcAQqov"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# Author: Lukasz Janyst <lukasz@jany.st>\n",
    "# Date:   27.08.2017\n",
    "#-------------------------------------------------------------------------------\n",
    "# This file is part of SSD-TensorFlow.\n",
    "#\n",
    "# SSD-TensorFlow is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# SSD-TensorFlow is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with SSD-Tensorflow.  If not, see <http://www.gnu.org/licenses/>.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm\n",
    "\n",
    "#-------Это класс для визуализации результатов----------------------------------\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "#------------------блок свертки  -------------------------------------------\n",
    "def conv_map(x, size, shape, stride, name, padding='SAME'):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable(\"filter\",\n",
    "                            shape=[shape, shape, x.get_shape()[3], size],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.Variable(tf.zeros(size), name='biases')\n",
    "        x = tf.nn.conv2d(x, w, strides=[1, stride, stride, 1], padding=padding)\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "        x = tf.nn.relu(x)\n",
    "        l2 = tf.nn.l2_loss(w)\n",
    "    return x, l2\n",
    "\n",
    "#---------------блок классификации--------------------------------------------------\n",
    "def classifier(x, size, mapsize, name):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable(\"filter\",\n",
    "                            shape=[3, 3, x.get_shape()[3], size],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.Variable(tf.zeros(size), name='biases')\n",
    "        x = tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "        x = tf.reshape(x, [-1, mapsize.w*mapsize.h, size])\n",
    "        l2 = tf.nn.l2_loss(w)\n",
    "    return x, l2\n",
    "\n",
    "#------------------------потери l1----------------------------------------------\n",
    "def smooth_l1_loss(x):\n",
    "    square_loss   = 0.5*x**2\n",
    "    absolute_loss = tf.abs(x)\n",
    "    return tf.where(tf.less(absolute_loss, 1.), square_loss, absolute_loss-0.5)\n",
    "\n",
    "#-----------------------массив тезоров-----------------------------------------------\n",
    "def array2tensor(x, name):\n",
    "    init = tf.constant_initializer(value=x, dtype=tf.float32)\n",
    "    tensor = tf.get_variable(name=name, initializer=init, shape=x.shape)\n",
    "    return tensor\n",
    "\n",
    "#----------------L2 потери-------------------------------------------------\n",
    "def l2_normalization(x, initial_scale, channels, name):\n",
    "    with tf.variable_scope(name):\n",
    "        scale = array2tensor(initial_scale*np.ones(channels), 'scale')\n",
    "        x = scale*tf.nn.l2_normalize(x, axis=-1)\n",
    "    return x\n",
    "\n",
    "#---------------------включаем сеть 16 в Детектор -----------------------------------------------\n",
    "class SSDVGG:\n",
    "    #---------------------------------------------------------------------------\n",
    "    def __init__(self, session, preset):\n",
    "        self.preset = preset\n",
    "        self.session = session\n",
    "        self.__built = False\n",
    "        self.__build_names()\n",
    "\n",
    "    #-------------------грузим сеть-----------------------------------------------\n",
    "    def build_from_vgg(self, vgg_dir, num_classes, a_trous=True,\n",
    "                       progress_hook='tqdm'):\n",
    "        \"\"\"\n",
    "        Build the model for training based on a pre-define vgg16 model.\n",
    "        :param vgg_dir:       directory where the vgg model should be stored\n",
    "        :param num_classes:   number of classes\n",
    "        :param progress_hook: a hook to show download progress of vgg16;\n",
    "                              the value may be a callable for urlretrieve\n",
    "                              or string \"tqdm\"\n",
    "        \"\"\"\n",
    "        self.num_classes = num_classes+1\n",
    "        self.num_vars = num_classes+5\n",
    "        self.l2_loss = 0\n",
    "        self.__download_vgg(vgg_dir, progress_hook)\n",
    "        self.__load_vgg(vgg_dir)\n",
    "        if a_trous: self.__build_vgg_mods_a_trous()\n",
    "        else: self.__build_vgg_mods()\n",
    "        self.__build_ssd_layers()\n",
    "        self.__build_norms()\n",
    "        self.__select_feature_maps()\n",
    "        self.__build_classifiers()\n",
    "        self.__built = True\n",
    "\n",
    "    #---------------строим -----------------------------------------------\n",
    "    def build_from_metagraph(self, metagraph_file, checkpoint_file):\n",
    "        \"\"\"\n",
    "        Build the model for inference from a metagraph shapshot and weights\n",
    "        checkpoint.\n",
    "        \"\"\"\n",
    "        sess = self.session\n",
    "        saver = tf.train.import_meta_graph(metagraph_file)\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "        self.image_input = sess.graph.get_tensor_by_name('image_input:0')\n",
    "        self.keep_prob   = sess.graph.get_tensor_by_name('keep_prob:0')\n",
    "        self.result      = sess.graph.get_tensor_by_name('result/result:0')\n",
    "\n",
    "    #-------------------потери оптимизатора по всем путям---------------------------------------\n",
    "    def build_optimizer_from_metagraph(self):\n",
    "        \"\"\"\n",
    "        Get the optimizer and the loss from metagraph\n",
    "        \"\"\"\n",
    "        sess = self.session\n",
    "        self.loss = sess.graph.get_tensor_by_name('total_loss/loss:0')\n",
    "        self.localization_loss = sess.graph.get_tensor_by_name('localization_loss/localization_loss:0')\n",
    "        self.confidence_loss = sess.graph.get_tensor_by_name('confidence_loss/confidence_loss:0')\n",
    "        self.l2_loss = sess.graph.get_tensor_by_name('total_loss/l2_loss:0')\n",
    "        self.optimizer = sess.graph.get_operation_by_name('optimizer/optimizer')\n",
    "        self.labels = sess.graph.get_tensor_by_name('labels:0')\n",
    "\n",
    "        self.losses = {\n",
    "            'total': self.loss,\n",
    "            'localization': self.localization_loss,\n",
    "            'confidence': self.confidence_loss,\n",
    "            'l2': self.l2_loss\n",
    "        }\n",
    "\n",
    "    #---------------------сохраняем сеть ------------------------------------------\n",
    "    def __download_vgg(self, vgg_dir, progress_hook):\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Check if the model needs to be downloaded\n",
    "        #-----------------------------------------------------------------------\n",
    "        vgg_archive = 'vgg.zip'\n",
    "        vgg_files   = [\n",
    "            vgg_dir + '/variables/variables.data-00000-of-00001',\n",
    "            vgg_dir + '/variables/variables.index',\n",
    "            vgg_dir + '/saved_model.pb']\n",
    "\n",
    "        missing_vgg_files = [vgg_file for vgg_file in vgg_files \\\n",
    "                             if not os.path.exists(vgg_file)]\n",
    "\n",
    "        if missing_vgg_files:\n",
    "            if os.path.exists(vgg_dir):\n",
    "                shutil.rmtree(vgg_dir)\n",
    "            os.makedirs(vgg_dir)\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Download vgg\n",
    "            #-------------------------------------------------------------------\n",
    "            url = 'https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/vgg.zip'\n",
    "            if not os.path.exists(vgg_archive):\n",
    "                if callable(progress_hook):\n",
    "                    urlretrieve(url, vgg_archive, progress_hook)\n",
    "                else:\n",
    "                    with DLProgress(unit='B', unit_scale=True, miniters=1) as pbar:\n",
    "                        urlretrieve(url, vgg_archive, pbar.hook)\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Extract vgg\n",
    "            #-------------------------------------------------------------------\n",
    "            zip_archive = zipfile.ZipFile(vgg_archive, 'r')\n",
    "            zip_archive.extractall(vgg_dir)\n",
    "            zip_archive.close()\n",
    "\n",
    "    #--------------грузим сеть ---------------------------------------------------\n",
    "    def __load_vgg(self, vgg_dir):\n",
    "        sess = self.session\n",
    "        graph = tf.saved_model.loader.load(sess, ['vgg16'], vgg_dir+'/vgg')\n",
    "        self.image_input = sess.graph.get_tensor_by_name('image_input:0')\n",
    "        self.keep_prob   = sess.graph.get_tensor_by_name('keep_prob:0')\n",
    "        self.vgg_conv4_3 = sess.graph.get_tensor_by_name('conv4_3/Relu:0')\n",
    "        self.vgg_conv5_3 = sess.graph.get_tensor_by_name('conv5_3/Relu:0')\n",
    "        self.vgg_fc6_w   = sess.graph.get_tensor_by_name('fc6/weights:0')\n",
    "        self.vgg_fc6_b   = sess.graph.get_tensor_by_name('fc6/biases:0')\n",
    "        self.vgg_fc7_w   = sess.graph.get_tensor_by_name('fc7/weights:0')\n",
    "        self.vgg_fc7_b   = sess.graph.get_tensor_by_name('fc7/biases:0')\n",
    "\n",
    "        layers = ['conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1',\n",
    "                  'conv3_2', 'conv3_3', 'conv4_1', 'conv4_2', 'conv4_3',\n",
    "                  'conv5_1', 'conv5_2', 'conv5_3']\n",
    "\n",
    "        for l in layers:\n",
    "            self.l2_loss += sess.graph.get_tensor_by_name(l+'/L2Loss:0')\n",
    "\n",
    "    #--------------------собираем сеть VGG16------------------------------------\n",
    "    def __build_vgg_mods(self):\n",
    "        self.mod_pool5 = tf.nn.max_pool(self.vgg_conv5_3, ksize=[1, 3, 3, 1],\n",
    "                                        strides=[1, 1, 1, 1], padding='SAME',\n",
    "                                        name='mod_pool5')\n",
    "\n",
    "        with tf.variable_scope('mod_conv6'):\n",
    "            x = tf.nn.conv2d(self.mod_pool5, self.vgg_fc6_w,\n",
    "                             strides=[1, 1, 1, 1], padding='SAME')\n",
    "            x = tf.nn.bias_add(x, self.vgg_fc6_b)\n",
    "            self.mod_conv6 = tf.nn.relu(x)\n",
    "            self.l2_loss += tf.nn.l2_loss(self.vgg_fc6_w)\n",
    "\n",
    "        with tf.variable_scope('mod_conv7'):\n",
    "            x = tf.nn.conv2d(self.mod_conv6, self.vgg_fc7_w,\n",
    "                             strides=[1, 1, 1, 1], padding='SAME')\n",
    "            x = tf.nn.bias_add(x, self.vgg_fc7_b)\n",
    "            x = tf.nn.relu(x)\n",
    "            self.mod_conv7 = x\n",
    "            self.l2_loss += tf.nn.l2_loss(self.vgg_fc7_w)\n",
    "\n",
    "    #------------------------пулинг------------------------------------\n",
    "    def __build_vgg_mods_a_trous(self):\n",
    "        sess = self.session\n",
    "\n",
    "        self.mod_pool5 = tf.nn.max_pool(self.vgg_conv5_3, ksize=[1, 3, 3, 1],\n",
    "                                        strides=[1, 1, 1, 1], padding='SAME',\n",
    "                                        name='mod_pool5')\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Modified conv6\n",
    "        #-----------------------------------------------------------------------\n",
    "        with tf.variable_scope('mod_conv6'):\n",
    "            #-------------------------------------------------------------------\n",
    "            # Decimate the weights\n",
    "            #-------------------------------------------------------------------\n",
    "            orig_w, orig_b = sess.run([self.vgg_fc6_w, self.vgg_fc6_b])\n",
    "            mod_w = np.zeros((3, 3, 512, 1024))\n",
    "            mod_b = np.zeros(1024)\n",
    "\n",
    "            for i in range(1024):\n",
    "                mod_b[i] = orig_b[4*i]\n",
    "                for h in range(3):\n",
    "                    for w in range(3):\n",
    "                        mod_w[h, w, :, i] = orig_w[3*h, 3*w, :, 4*i]\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Строим карты Build the feature map\n",
    "            #-------------------------------------------------------------------\n",
    "            w = array2tensor(mod_w, 'filter')\n",
    "            b = array2tensor(mod_b, 'biases')\n",
    "            x = tf.nn.atrous_conv2d(self.mod_pool5, w, rate=6, padding='SAME')\n",
    "            x = tf.nn.bias_add(x, b)\n",
    "            x = tf.nn.relu(x)\n",
    "            self.mod_conv6 = x\n",
    "            self.l2_loss += tf.nn.l2_loss(w)\n",
    "\n",
    "        #--------------------свертки----------------------------------------\n",
    "        # Modified conv7\n",
    "        #-----------------------------------------------------------------------\n",
    "        with tf.variable_scope('mod_conv7'):\n",
    "            #-------------------------------------------------------------------\n",
    "            # Decimate the weights\n",
    "            #-------------------------------------------------------------------\n",
    "            orig_w, orig_b = sess.run([self.vgg_fc7_w, self.vgg_fc7_b])\n",
    "            mod_w = np.zeros((1, 1, 1024, 1024))\n",
    "            mod_b = np.zeros(1024)\n",
    "\n",
    "            for i in range(1024):\n",
    "                mod_b[i] = orig_b[4*i]\n",
    "                for j in range(1024):\n",
    "                    mod_w[:, :, j, i] = orig_w[:, :, 4*j, 4*i]\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Build the feature map\n",
    "            #-------------------------------------------------------------------\n",
    "            w = array2tensor(mod_w, 'filter')\n",
    "            b = array2tensor(mod_b, 'biases')\n",
    "            x = tf.nn.conv2d(self.mod_conv6, w, strides=[1, 1, 1, 1],\n",
    "                             padding='SAME')\n",
    "            x = tf.nn.bias_add(x, b)\n",
    "            x = tf.nn.relu(x)\n",
    "            self.mod_conv7 = x\n",
    "            self.l2_loss += tf.nn.l2_loss(w)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def __with_loss(self, x, l2_loss):\n",
    "        self.l2_loss += l2_loss\n",
    "        return x\n",
    "\n",
    "    #-----------------слои SSD-----------------------------------------------\n",
    "    def __build_ssd_layers(self):\n",
    "        stride10 = 1\n",
    "        padding10 = 'VALID'\n",
    "        if len(self.preset.maps) >= 7:\n",
    "            stride10 = 2\n",
    "            padding10 = 'SAME'\n",
    "\n",
    "        x, l2  = conv_map(self.mod_conv7,    256, 1, 1, 'conv8_1')\n",
    "        self.ssd_conv8_1 = self.__with_loss(x, l2)\n",
    "        x, l2 = conv_map(self.ssd_conv8_1,  512, 3, 2, 'conv8_2')\n",
    "        self.ssd_conv8_2 = self.__with_loss(x, l2)\n",
    "        x, l2  = conv_map(self.ssd_conv8_2,  128, 1, 1, 'conv9_1')\n",
    "        self.ssd_conv9_1 = self.__with_loss(x, l2)\n",
    "        x, l2 = conv_map(self.ssd_conv9_1,  256, 3, 2, 'conv9_2')\n",
    "        self.ssd_conv9_2 = self.__with_loss(x, l2)\n",
    "        x, l2 = conv_map(self.ssd_conv9_2,  128, 1, 1, 'conv10_1')\n",
    "        self.ssd_conv10_1 = self.__with_loss(x, l2)\n",
    "        x, l2 = conv_map(self.ssd_conv10_1, 256, 3, stride10, 'conv10_2', padding10)\n",
    "        self.ssd_conv10_2 = self.__with_loss(x, l2)\n",
    "        x, l2 = conv_map(self.ssd_conv10_2, 128, 1, 1, 'conv11_1')\n",
    "        self.ssd_conv11_1 = self.__with_loss(x, l2)\n",
    "        x, l2 = conv_map(self.ssd_conv11_1, 256, 3, 1, 'conv11_2', 'VALID')\n",
    "        self.ssd_conv11_2 = self.__with_loss(x, l2)\n",
    "\n",
    "        if len(self.preset.maps) < 7:\n",
    "            return\n",
    "\n",
    "        x, l2 = conv_map(self.ssd_conv11_2, 128, 1, 1, 'conv12_1')\n",
    "        paddings = [[0, 0], [0, 1], [0, 1], [0, 0]]\n",
    "        x = tf.pad(x, paddings, \"CONSTANT\")\n",
    "        self.ssd_conv12_1 = self.__with_loss(x, l2)\n",
    "        x, l2 = conv_map(self.ssd_conv12_1, 256, 3, 1, 'conv12_2', 'VALID')\n",
    "        self.ssd_conv12_2 = self.__with_loss(x, l2)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def __build_norms(self):\n",
    "        x = l2_normalization(self.vgg_conv4_3, 20, 512, 'l2_norm_conv4_3')\n",
    "        self.norm_conv4_3 = x\n",
    "\n",
    "    #---------------------карты признаков для работы SSD-------------------------------------------\n",
    "    def __select_feature_maps(self):\n",
    "        self.__maps = [\n",
    "            self.norm_conv4_3,\n",
    "            self.mod_conv7,\n",
    "            self.ssd_conv8_2,\n",
    "            self.ssd_conv9_2,\n",
    "            self.ssd_conv10_2,\n",
    "            self.ssd_conv11_2]\n",
    "\n",
    "        if len(self.preset.maps) == 7:\n",
    "            self.__maps.append(self.ssd_conv12_2)\n",
    "\n",
    "    #----------------------классификации для SSD------------------------------------\n",
    "    def __build_classifiers(self):\n",
    "        with tf.variable_scope('classifiers'):\n",
    "            self.__classifiers = []\n",
    "            for i in range(len(self.__maps)):\n",
    "                fmap = self.__maps[i]\n",
    "                map_size = self.preset.maps[i].size\n",
    "                for j in range(2+len(self.preset.maps[i].aspect_ratios)):\n",
    "                    name = 'classifier{}_{}'.format(i, j)\n",
    "                    clsfier, l2 = classifier(fmap, self.num_vars, map_size, name)\n",
    "                    self.__classifiers.append(self.__with_loss(clsfier, l2))\n",
    "\n",
    "        with tf.variable_scope('output'):\n",
    "            output      = tf.concat(self.__classifiers, axis=1, name='output')\n",
    "            self.logits = output[:,:,:self.num_classes]\n",
    "\n",
    "        with tf.variable_scope('result'):\n",
    "            self.classifier = tf.nn.softmax(self.logits)\n",
    "            self.locator    = output[:,:,self.num_classes:]\n",
    "            self.result     = tf.concat([self.classifier, self.locator],\n",
    "                                        axis=-1, name='result')\n",
    "\n",
    "    #-----------------------Строим оценку потерь----------------------------------------\n",
    "    def build_optimizer(self, learning_rate=0.001, weight_decay=0.0005,\n",
    "                        momentum=0.9, global_step=None):\n",
    "\n",
    "        self.labels = tf.placeholder(tf.float32, name='labels',\n",
    "                                    shape=[None, None, self.num_vars])\n",
    "\n",
    "        with tf.variable_scope('ground_truth'):\n",
    "            #-------------------------------------------------------------------\n",
    "            # Split the ground truth tensor\n",
    "            #-------------------------------------------------------------------\n",
    "            # Classification ground truth tensor\n",
    "            # Shape: (batch_size, num_anchors, num_classes)\n",
    "            gt_cl = self.labels[:,:,:self.num_classes]\n",
    "\n",
    "            # Localization ground truth tensor\n",
    "            # Shape: (batch_size, num_anchors, 4)\n",
    "            gt_loc = self.labels[:,:,self.num_classes:]\n",
    "\n",
    "            # Batch size\n",
    "            # Shape: scalar\n",
    "            batch_size = tf.shape(gt_cl)[0]\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Compute match counters\n",
    "        #-----------------------------------------------------------------------\n",
    "        with tf.variable_scope('match_counters'):\n",
    "            # Number of anchors per sample\n",
    "            # Shape: (batch_size)\n",
    "            total_num = tf.ones([batch_size], dtype=tf.int64) * \\\n",
    "                        tf.to_int64(self.preset.num_anchors)\n",
    "\n",
    "            # Number of negative (not-matched) anchors per sample, computed\n",
    "            # by counting boxes of the background class in each sample.\n",
    "            # Shape: (batch_size)\n",
    "            negatives_num = tf.count_nonzero(gt_cl[:,:,-1], axis=1)\n",
    "\n",
    "            # Number of positive (matched) anchors per sample\n",
    "            # Shape: (batch_size)\n",
    "            positives_num = total_num-negatives_num\n",
    "\n",
    "            # Number of positives per sample that is division-safe\n",
    "            # Shape: (batch_size)\n",
    "            positives_num_safe = tf.where(tf.equal(positives_num, 0),\n",
    "                                          tf.ones([batch_size])*10e-15,\n",
    "                                          tf.to_float(positives_num))\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Compute masks\n",
    "        #-----------------------------------------------------------------------\n",
    "        with tf.variable_scope('match_masks'):\n",
    "            # Boolean tensor determining whether an anchor is a positive\n",
    "            # Shape: (batch_size, num_anchors)\n",
    "            positives_mask = tf.equal(gt_cl[:,:,-1], 0)\n",
    "\n",
    "            # Boolean tensor determining whether an anchor is a negative\n",
    "            # Shape: (batch_size, num_anchors)\n",
    "            negatives_mask = tf.logical_not(positives_mask)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Compute the confidence loss\n",
    "        #-----------------------------------------------------------------------\n",
    "        with tf.variable_scope('confidence_loss'):\n",
    "            # Cross-entropy tensor - all of the values are non-negative\n",
    "            # Shape: (batch_size, num_anchors)\n",
    "            ce = tf.nn.softmax_cross_entropy_with_logits_v2(labels=gt_cl,\n",
    "                                                            logits=self.logits)\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Sum up the loss of all the positive anchors\n",
    "            #-------------------------------------------------------------------\n",
    "            # Positives - the loss of negative anchors is zeroed out\n",
    "            # Shape: (batch_size, num_anchors)\n",
    "            positives = tf.where(positives_mask, ce, tf.zeros_like(ce))\n",
    "\n",
    "            # Total loss of positive anchors\n",
    "            # Shape: (batch_size)\n",
    "            positives_sum = tf.reduce_sum(positives, axis=-1)\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Figure out what the negative anchors with highest confidence loss\n",
    "            # are\n",
    "            #-------------------------------------------------------------------\n",
    "            # Negatives - the loss of positive anchors is zeroed out\n",
    "            # Shape: (batch_size, num_anchors)\n",
    "            negatives = tf.where(negatives_mask, ce, tf.zeros_like(ce))\n",
    "\n",
    "            # Top negatives - sorted confience loss with the highest one first\n",
    "            # Shape: (batch_size, num_anchors)\n",
    "            negatives_top = tf.nn.top_k(negatives, self.preset.num_anchors)[0]\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Fugure out what the number of negatives we want to keep is\n",
    "            #-------------------------------------------------------------------\n",
    "            # Maximum number of negatives to keep per sample - we keep at most\n",
    "            # 3 times as many as we have positive anchors in the sample\n",
    "            # Shape: (batch_size)\n",
    "            negatives_num_max = tf.minimum(negatives_num, 3*positives_num)\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Mask out superfluous negatives and compute the sum of the loss\n",
    "            #-------------------------------------------------------------------\n",
    "            # Transposed vector of maximum negatives per sample\n",
    "            # Shape (batch_size, 1)\n",
    "            negatives_num_max_t = tf.expand_dims(negatives_num_max, 1)\n",
    "\n",
    "            # Range tensor: [0, 1, 2, ..., num_anchors-1]\n",
    "            # Shape: (num_anchors)\n",
    "            rng = tf.range(0, self.preset.num_anchors, 1)\n",
    "\n",
    "            # Row range, the same as above, but int64 and a row of a matrix\n",
    "            # Shape: (1, num_anchors)\n",
    "            range_row = tf.to_int64(tf.expand_dims(rng, 0))\n",
    "\n",
    "            # Mask of maximum negatives - first `negative_num_max` elements\n",
    "            # in corresponding row are `True`, the rest is false\n",
    "            # Shape: (batch_size, num_anchors)\n",
    "            negatives_max_mask = tf.less(range_row, negatives_num_max_t)\n",
    "\n",
    "            # Max negatives - all the positives and superfluous negatives are\n",
    "            # zeroed out.\n",
    "            # Shape: (batch_size, num_anchors)\n",
    "            negatives_max = tf.where(negatives_max_mask, negatives_top,\n",
    "                                     tf.zeros_like(negatives_top))\n",
    "\n",
    "            # Sum of max negatives for each sample\n",
    "            # Shape: (batch_size)\n",
    "            negatives_max_sum = tf.reduce_sum(negatives_max, axis=-1)\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Compute the confidence loss for each element\n",
    "            #-------------------------------------------------------------------\n",
    "            # Total confidence loss for each sample\n",
    "            # Shape: (batch_size)\n",
    "            confidence_loss = tf.add(positives_sum, negatives_max_sum)\n",
    "\n",
    "            # Total confidence loss normalized by the number of positives\n",
    "            # per sample\n",
    "            # Shape: (batch_size)\n",
    "            confidence_loss = tf.where(tf.equal(positives_num, 0),\n",
    "                                       tf.zeros([batch_size]),\n",
    "                                       tf.div(confidence_loss,\n",
    "                                              positives_num_safe))\n",
    "\n",
    "            # Mean confidence loss for the batch\n",
    "            # Shape: scalar\n",
    "            self.confidence_loss = tf.reduce_mean(confidence_loss,\n",
    "                                                  name='confidence_loss')\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Compute the localization loss\n",
    "        #-----------------------------------------------------------------------\n",
    "        with tf.variable_scope('localization_loss'):\n",
    "            # Element-wise difference between the predicted localization loss\n",
    "            # and the ground truth\n",
    "            # Shape: (batch_size, num_anchors, 4)\n",
    "            loc_diff = tf.subtract(self.locator, gt_loc)\n",
    "\n",
    "            # Smooth L1 loss\n",
    "            # Shape: (batch_size, num_anchors, 4)\n",
    "            loc_loss = smooth_l1_loss(loc_diff)\n",
    "\n",
    "            # Sum of localization losses for each anchor\n",
    "            # Shape: (batch_size, num_anchors)\n",
    "            loc_loss_sum = tf.reduce_sum(loc_loss, axis=-1)\n",
    "\n",
    "            # Positive locs - the loss of negative anchors is zeroed out\n",
    "            # Shape: (batch_size, num_anchors)\n",
    "            positive_locs = tf.where(positives_mask, loc_loss_sum,\n",
    "                                     tf.zeros_like(loc_loss_sum))\n",
    "\n",
    "            # Total loss of positive anchors\n",
    "            # Shape: (batch_size)\n",
    "            localization_loss = tf.reduce_sum(positive_locs, axis=-1)\n",
    "\n",
    "            # Total localization loss normalized by the number of positives\n",
    "            # per sample\n",
    "            # Shape: (batch_size)\n",
    "            localization_loss = tf.where(tf.equal(positives_num, 0),\n",
    "                                         tf.zeros([batch_size]),\n",
    "                                         tf.div(localization_loss,\n",
    "                                                positives_num_safe))\n",
    "\n",
    "            # Mean localization loss for the batch\n",
    "            # Shape: scalar\n",
    "            self.localization_loss = tf.reduce_mean(localization_loss,\n",
    "                                                    name='localization_loss')\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Compute total loss\n",
    "        #-----------------------------------------------------------------------\n",
    "        with tf.variable_scope('total_loss'):\n",
    "            # Sum of the localization and confidence loss\n",
    "            # Shape: (batch_size)\n",
    "            self.conf_and_loc_loss = tf.add(self.confidence_loss,\n",
    "                                            self.localization_loss,\n",
    "                                            name='sum_losses')\n",
    "\n",
    "            # L2 loss\n",
    "            # Shape: scalar\n",
    "            self.l2_loss = tf.multiply(weight_decay, self.l2_loss,\n",
    "                                       name='l2_loss')\n",
    "\n",
    "            # Final loss\n",
    "            # Shape: scalar\n",
    "            self.loss = tf.add(self.conf_and_loc_loss, self.l2_loss,\n",
    "                               name='loss')\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Build the optimizer\n",
    "        #-----------------------------------------------------------------------\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "            optimizer = optimizer.minimize(self.loss, global_step=global_step,\n",
    "                                           name='optimizer')\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Store the tensors\n",
    "        #-----------------------------------------------------------------------\n",
    "        self.optimizer = optimizer\n",
    "        self.losses = {\n",
    "            'total': self.loss,\n",
    "            'localization': self.localization_loss,\n",
    "            'confidence': self.confidence_loss,\n",
    "            'l2': self.l2_loss\n",
    "        }\n",
    "\n",
    "    #-----------------------задаем имена слоев-------------------------------------------\n",
    "    def __build_names(self):\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Names of the original and new scopes\n",
    "        #-----------------------------------------------------------------------\n",
    "        self.original_scopes = [\n",
    "            'conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1', 'conv3_2',\n",
    "            'conv3_3', 'conv4_1', 'conv4_2', 'conv4_3', 'conv5_1', 'conv5_2',\n",
    "            'conv5_3', 'mod_conv6', 'mod_conv7'\n",
    "        ]\n",
    "\n",
    "        self.new_scopes = [\n",
    "            'conv8_1', 'conv8_2', 'conv9_1', 'conv9_2', 'conv10_1', 'conv10_2',\n",
    "            'conv11_1', 'conv11_2'\n",
    "        ]\n",
    "\n",
    "        if len(self.preset.maps) == 7:\n",
    "            self.new_scopes += ['conv12_1', 'conv12_2']\n",
    "\n",
    "        for i in range(len(self.preset.maps)):\n",
    "            for j in range(2+len(self.preset.maps[i].aspect_ratios)):\n",
    "                self.new_scopes.append('classifiers/classifier{}_{}'.format(i, j))\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def build_summaries(self, restore):\n",
    "        if restore:\n",
    "            return self.session.graph.get_tensor_by_name('net_summaries/net_summaries:0')\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Build the filter summaries\n",
    "        #-----------------------------------------------------------------------\n",
    "        names = self.original_scopes + self.new_scopes\n",
    "        sess = self.session\n",
    "        with tf.variable_scope('filter_summaries'):\n",
    "            summaries = []\n",
    "            for name in names:\n",
    "                tensor = sess.graph.get_tensor_by_name(name+'/filter:0')\n",
    "                summary = tf.summary.histogram(name, tensor)\n",
    "                summaries.append(summary)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Scale summary\n",
    "        #-----------------------------------------------------------------------\n",
    "        with tf.variable_scope('scale_summary'):\n",
    "            tensor = sess.graph.get_tensor_by_name('l2_norm_conv4_3/scale:0')\n",
    "            summary = tf.summary.histogram('l2_norm_conv4_3', tensor)\n",
    "            summaries.append(summary)\n",
    "\n",
    "        return tf.summary.merge(summaries, name='net_summaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJ_l9W8M5ePN"
   },
   "source": [
    "## Практическое задание\n",
    "\n",
    "\n",
    "<ol>\n",
    "    <li>Сделайте краткий обзор какой-нибудь научной работы посвященной тому или иному алгоритму для object detection, который не рассматривался на уроке. Проведите анализ: Чем отличается выбранная вами на рассмотрение архитектура нейронной сети от других архитектур? В чем плюсы и минусы данной архитектуры? Какие могут возникнуть трудности при применении данной архитектуры на практике?\n",
    "    </li> \n",
    "    <li>Запустите детектор (ssdMobile_v2 или faster_rcnn, или любой другой детектор) для своей картинки и попробуйте найти 10 объектов, 100 объектов.\n",
    "    </li>\n",
    "    <li>* Ссылка на репозиторий с полным кодом для обучения ssd нейросети - https://github.com/sergeyveneckiy/ssd-tensorflow. Попробуйте улучшить точность ее работы и напишите отчет, что вы пробовали изменить в ее параметрах и как это отражалось на процессе обучения нейронной сети. \n",
    "        Обратите внимание! Мин. сист. требования для запуска данного проекта - это минимум 8 Gb ОЗУ. Если у вас недостаточно мощности компьютера, то вы можете просто изучить содержимое исходного кода и датасета данного проекта.</li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiGyjf1M5ePP"
   },
   "source": [
    "## Дополнительные материалы\n",
    "\n",
    "<ol>\n",
    "    <li>Оригинальная научная статья по MS COCO dataset - https://arxiv.org/pdf/1405.0312.pdf</li>\n",
    "    <li>Оригинальная научная статья по R-CNN - https://arxiv.org/pdf/1311.2524.pdf</li>\n",
    "    <li>Оригинальная научная статья по Fast R-CNN - https://arxiv.org/pdf/1504.08083.pdf</li>\n",
    "     <li>Оригинальная научная статья по Faster R-CNN - https://arxiv.org/pdf/1506.01497.pdf</li>\n",
    "    <li>Оригинальная научная статья по YOLO - https://arxiv.org/pdf/1506.02640.pdf</li>\n",
    "    <li>Оригинальная научная статья по SSD - https://arxiv.org/pdf/1512.02325.pdf</li>\n",
    "     <li>Оригинальная научная статья по Mask R-CNN - https://arxiv.org/pdf/2001.05566.pdf</li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnpfYxC15ePQ"
   },
   "source": [
    "## Используемая литература \n",
    "\n",
    "Для подготовки данного методического пособия были использованы следующие ресурсы:\n",
    "<ol>\n",
    "    <li>https://github.com/ljanyst/ssd-tensorflow</li>\n",
    "    <li>Recent Advances in Deep Learning for Object Detection. Xiongwei Wu, Doyen Sahoo, Steven C.H. Hoi. 2019</li>    \n",
    "    <li>Object Detection with Deep Learning: A Review. Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, Xindong Wu. 2019\n",
    "</li>\n",
    "    <li>Object Detection in 20 Years: A Survey. Zhengxia Zou (1), Zhenwei Shi (2), Yuhong Guo (3 and 4), Jieping Ye (1 and 4) ((1) University of Michigan, (2) Beihang University, (3) Carleton University, (4) DiDi Chuxing). 2019</li>\n",
    "    <li>Википедия</li>  \n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "7less_obj_dect.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
