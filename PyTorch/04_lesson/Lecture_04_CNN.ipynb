{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "Напомню, свертки - это то, с чего начался хайп нейронных сетей в районе 2012-ого.\n",
    "\n",
    "Работают они примерно так:  \n",
    "![Conv example](https://image.ibb.co/e6t8ZK/Convolution.gif)   \n",
    "From [Feature extraction using convolution](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution).\n",
    "\n",
    "Формально - учатся наборы фильтров, каждый из которых скалярно умножается на элементы матрицы признаков. На картинке выше исходная матрица сворачивается с фильтром\n",
    "$$\n",
    " \\begin{pmatrix}\n",
    "  1 & 0 & 1 \\\\\n",
    "  0 & 1 & 0 \\\\\n",
    "  1 & 0 & 1\n",
    " \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Но нужно не забывать, что свертки обычно имеют ещё такую размерность, как число каналов. Например, картинки имеют обычно три канала: RGB.  \n",
    "Наглядно демонстрируется как выглядят при этом фильтры [здесь](http://cs231n.github.io/convolutional-networks/#conv).\n",
    "\n",
    "После сверток обычно следуют pooling-слои. Они помогают уменьшить размерность тензора, с которым приходится работать. Самым частым является max-pooling:  \n",
    "![maxpooling](http://cs231n.github.io/assets/cnn/maxpool.jpeg)  \n",
    "From [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/#pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Свёртки для текстов\n",
    "\n",
    "Для текстов свертки работают как n-граммные детекторы (примерно). Каноничный пример символьной сверточной сети:\n",
    "\n",
    "![text-convs](https://image.ibb.co/bC3Xun/2018_03_27_01_24_39.png)  \n",
    "From [Character-Aware Neural Language Models](https://arxiv.org/abs/1508.06615)\n",
    "\n",
    "*Сколько учится фильтров на данном примере?*\n",
    "\n",
    "На картинке показано, как из слова извлекаются 2, 3 и 4-граммы. Например, желтые - это триграммы. Желтый фильтр прикладывают ко всем триграммам в слове, а потом с помощью global max-pooling извлекают наиболее сильный сигнал.\n",
    "\n",
    "Что это значит, если конкретнее?\n",
    "\n",
    "Каждый символ отображается с помощью эмбеддингов в некоторый вектор. А их последовательности - в конкатенации эмбеддингов.  \n",
    "Например, \"abs\" $\\to [v_a; v_b; v_s] \\in \\mathbb{R}^{3 d}$, где $d$ - размерность эмбеддинга. Желтый фильтр $f_k$ имеет такую же размерность $3d$.  \n",
    "Его прикладывание - это скалярное произведение $\\left([v_a; v_b; v_s] \\odot f_k \\right) \\in \\mathbb R$ (один из желтых квадратиков в feature map для данного фильтра).\n",
    "\n",
    "Max-pooling выбирает $max_i \\left( [v_{i-1}; v_{i}; v_{i+1}] \\odot f_k \\right)$, где $i$ пробегается по всем индексам слова от 1 до $|w| - 1$ (либо по большему диапазону, если есть padding'и).   \n",
    "Этот максимум соответствует той триграмме, которая наиболее близка к фильтру по косинусному расстоянию.\n",
    "\n",
    "В результате в векторе после max-pooling'а закодирована информация о том, какие из n-грамм встретились в слове: если встретилась близкая к нашему $f_k$ триграмма, то в $k$-той позиции вектора будет стоять большое значение, иначе - маленькое.\n",
    "\n",
    "А учим мы как раз фильтры. То есть сеть должна научиться определять, какие из n-грамм значимы, а какие - нет.\n",
    "\n",
    "\n",
    "## Основные параметры сверток\n",
    "\n",
    "### Stride\n",
    "\n",
    "Stride - это \"шаг\" с которым идем по матрице объекта\n",
    "\n",
    "<img src=\"images/strides.gif\">\n",
    "\n",
    "### Dilation\n",
    "\n",
    "Dilation - расстояние между точками ядра\n",
    "\n",
    "<img src=\"images/dilations.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.CIFAR10(root='data/', train=True, download=True)\n",
    "\n",
    "def train_valid_split(Xt):\n",
    "    X_train, X_test = train_test_split(Xt, test_size=0.05, random_state=13)\n",
    "    return X_train, X_test\n",
    "\n",
    "class MyOwnCifar(torch.utils.data.Dataset):\n",
    "   \n",
    "    def __init__(self, init_dataset, transform=None):\n",
    "        self._base_dataset = init_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self._base_dataset[idx][0]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self._base_dataset[idx][1]\n",
    "    \n",
    "trans_actions = transforms.Compose([transforms.Scale(44),\n",
    "                                    transforms.RandomCrop(32, padding=4), \n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "train_dataset, valid_dataset = train_valid_split(dataset)\n",
    "\n",
    "train_dataset = MyOwnCifar(train_dataset, trans_actions)\n",
    "valid_dataset = MyOwnCifar(valid_dataset, transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=True,\n",
    "                          num_workers=3)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=False,\n",
    "                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (dp_three): Dropout(p=0.2, inplace=False)\n",
      "  (dp_four): Dropout(p=0.2, inplace=False)\n",
      "  (bn_one): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_one): Conv2d(3, 30, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn_two): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_two): Conv2d(30, 60, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn_three): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_three): Conv2d(60, 120, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn_four): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=480, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.dp_three = nn.Dropout(0.2)\n",
    "        self.dp_four = nn.Dropout(0.2)\n",
    "        \n",
    "        self.bn_one = torch.nn.BatchNorm2d(3) \n",
    "        self.conv_one = torch.nn.Conv2d(3, 30, 3)\n",
    "        self.bn_two = torch.nn.BatchNorm2d(30) \n",
    "        self.conv_two = torch.nn.Conv2d(30, 60, 3)\n",
    "        self.bn_three = torch.nn.BatchNorm2d(60)\n",
    "        self.conv_three = torch.nn.Conv2d(60, 120, 3)\n",
    "        self.bn_four = torch.nn.BatchNorm2d(120)\n",
    "        self.fc1 = torch.nn.Linear(480, 200)\n",
    "        self.fc2 = torch.nn.Linear(200, 60)\n",
    "        self.out = torch.nn.Linear(60, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn_one(x)\n",
    "        x = self.conv_one(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.bn_two(x)\n",
    "        x = self.conv_two(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.bn_three(x)\n",
    "        x = self.conv_three(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.bn_four(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dp_three(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp_four(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return self.out(x)\n",
    "       \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(10)):  \n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0], data[1]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    net.eval()\n",
    "    loss_accumed = 0\n",
    "    for X, y in valid_loader:\n",
    "        output = net(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss_accumed += loss\n",
    "    print(\"Epoch {} valid_loss {}\".format(epoch, loss_accumed))\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN проблемы:\n",
    "\n",
    "1. Проблема сложность обобщения картинки\n",
    "\n",
    "Первая продвинутая сверточная архитектура - Alexnet\n",
    "\n",
    "<img src=\"images/alexnet.png\">\n",
    "\n",
    "Развитие - более тяжелые сети VGG\n",
    "\n",
    "<img src=\"images/vgg.png\">\n",
    "\n",
    "<img src=\"images/vgg2.jpg\">\n",
    "\n",
    "2. Проблема затухания градиентов - resnet, inception\n",
    "\n",
    "<img src=\"images/resnet.png\">\n",
    "\n",
    "<img src=\"images/inception_big.png\">\n",
    "<img src=\"images/inception.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "resnet18 = models.resnet18()\n",
    "alexnet = models.alexnet()\n",
    "vgg16 = models.vgg16()\n",
    "#inception = models.inception_v3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "#inception = models.inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Необходимые трансформации\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), normalize, transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но в жизни часто нужно решать какую-то свою задачу, а не ImageNet\n",
    "\n",
    "<img src=\"images/frozen.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_parameter_requires_grad(resnet18, True)\n",
    "resnet18.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinetik/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:219: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "trans_actions = transforms.Compose([transforms.Scale(256),\n",
    "                                    transforms.RandomCrop(224, padding=4),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                         std=[0.229, 0.224, 0.225])])\n",
    "valid_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                         std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset, valid_dataset = train_valid_split(dataset)\n",
    "\n",
    "train_dataset = MyOwnCifar(train_dataset, trans_actions)\n",
    "valid_dataset = MyOwnCifar(valid_dataset, valid_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=True,\n",
    "                          num_workers=3)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=False,\n",
    "                          num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0421, -0.0419, -0.0047,  ...,  0.0155,  0.0167, -0.0164],\n",
       "         [-0.0133,  0.0344, -0.0032,  ...,  0.0070, -0.0329, -0.0011],\n",
       "         [-0.0331,  0.0070, -0.0049,  ..., -0.0315, -0.0223, -0.0157],\n",
       "         ...,\n",
       "         [-0.0036,  0.0026, -0.0151,  ...,  0.0114,  0.0025,  0.0406],\n",
       "         [-0.0141, -0.0015, -0.0282,  ..., -0.0288, -0.0038,  0.0260],\n",
       "         [ 0.0320,  0.0098, -0.0413,  ...,  0.0231,  0.0095, -0.0144]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([-1.3775e-02, -3.4946e-02,  1.8888e-02,  3.3867e-05,  2.0739e-02,\n",
       "         -4.1647e-02,  3.5258e-02,  1.6097e-02, -1.5240e-02,  2.0511e-02],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_to_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = []\n",
    "for name,param in resnet18.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(10)):  \n",
    "    resnet18.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0], data[1]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = resnet18(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    resnet18.eval()\n",
    "    loss_accumed = 0\n",
    "    for X, y in valid_loader:\n",
    "        output = resnet18(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss_accumed += loss\n",
    "    print(\"Epoch {} valid_loss {}\".format(epoch, loss_accumed))\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
